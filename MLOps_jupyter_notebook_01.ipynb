{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " # 1. Setup Watson Machine Learning Connection"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "PROJECT_ID = 'xxx4-2daee35ddda6'\nAPI_KEY = 'Cpk3k0Sz8T96MesIzlFTtq-O8xKuQPiVs4gPqaN_ZusH'\nLOCATION = 'https://us-south.ml.cloud.ibm.com'"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\n    \"apikey\": API_KEY,\n    \"url\": LOCATION\n}\n\nwml_client = APIClient(wml_credentials)\n\nwml_client.set.default_space('d15f4a9e-e1f5-4bc2-ae89-6606bd2b62c8')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'SUCCESS'"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "wml_client.set.default_project(PROJECT_ID)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 2. Setting up DB2 Connection"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport tensorflow as tf\nimport keras\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CLIENTNUM</th>\n      <th>ATTRITION_FLAG</th>\n      <th>CUSTOMER_AGE</th>\n      <th>GENDER</th>\n      <th>DEPENDENT_COUNT</th>\n      <th>EDUCATION_LEVEL</th>\n      <th>MARITAL_STATUS</th>\n      <th>INCOME_CATEGORY</th>\n      <th>CARD_CATEGORY</th>\n      <th>MONTHS_ON_BOOK</th>\n      <th>...</th>\n      <th>MONTHS_INACTIVE_12_MON</th>\n      <th>CONTACTS_COUNT_12_MON</th>\n      <th>CREDIT_LIMIT</th>\n      <th>TOTAL_REVOLVING_BAL</th>\n      <th>AVG_OPEN_TO_BUY</th>\n      <th>TOTAL_AMT_CHNG_Q4_Q1</th>\n      <th>TOTAL_TRANS_AMT</th>\n      <th>TOTAL_TRANS_CT</th>\n      <th>TOTAL_CT_CHNG_Q4_Q1</th>\n      <th>AVG_UTILIZATION_RATIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>768805383</td>\n      <td>Existing Customer</td>\n      <td>45</td>\n      <td>M</td>\n      <td>3</td>\n      <td>High School</td>\n      <td>Married</td>\n      <td>$60K - $80K</td>\n      <td>Blue</td>\n      <td>39</td>\n      <td>...</td>\n      <td>1</td>\n      <td>3</td>\n      <td>12691.0</td>\n      <td>777</td>\n      <td>11914.0</td>\n      <td>1.335</td>\n      <td>1144</td>\n      <td>42</td>\n      <td>1.625</td>\n      <td>0.061</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>818770008</td>\n      <td>Existing Customer</td>\n      <td>49</td>\n      <td>F</td>\n      <td>5</td>\n      <td>Graduate</td>\n      <td>Single</td>\n      <td>Less than $40K</td>\n      <td>Blue</td>\n      <td>44</td>\n      <td>...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>8256.0</td>\n      <td>864</td>\n      <td>7392.0</td>\n      <td>1.541</td>\n      <td>1291</td>\n      <td>33</td>\n      <td>3.714</td>\n      <td>0.105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>713982108</td>\n      <td>Existing Customer</td>\n      <td>51</td>\n      <td>M</td>\n      <td>3</td>\n      <td>Graduate</td>\n      <td>Married</td>\n      <td>$80K - $120K</td>\n      <td>Blue</td>\n      <td>36</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3418.0</td>\n      <td>0</td>\n      <td>3418.0</td>\n      <td>2.594</td>\n      <td>1887</td>\n      <td>20</td>\n      <td>2.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>769911858</td>\n      <td>Existing Customer</td>\n      <td>40</td>\n      <td>F</td>\n      <td>4</td>\n      <td>High School</td>\n      <td>Unknown</td>\n      <td>Less than $40K</td>\n      <td>Blue</td>\n      <td>34</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>3313.0</td>\n      <td>2517</td>\n      <td>796.0</td>\n      <td>1.405</td>\n      <td>1171</td>\n      <td>20</td>\n      <td>2.333</td>\n      <td>0.760</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>709106358</td>\n      <td>Existing Customer</td>\n      <td>40</td>\n      <td>M</td>\n      <td>3</td>\n      <td>Uneducated</td>\n      <td>Married</td>\n      <td>$60K - $80K</td>\n      <td>Blue</td>\n      <td>21</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4716.0</td>\n      <td>0</td>\n      <td>4716.0</td>\n      <td>2.175</td>\n      <td>816</td>\n      <td>28</td>\n      <td>2.500</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 21 columns</p>\n</div>",
                        "text/plain": "   CLIENTNUM     ATTRITION_FLAG  CUSTOMER_AGE GENDER  DEPENDENT_COUNT  \\\n0  768805383  Existing Customer            45      M                3   \n1  818770008  Existing Customer            49      F                5   \n2  713982108  Existing Customer            51      M                3   \n3  769911858  Existing Customer            40      F                4   \n4  709106358  Existing Customer            40      M                3   \n\n  EDUCATION_LEVEL MARITAL_STATUS INCOME_CATEGORY CARD_CATEGORY  \\\n0     High School        Married     $60K - $80K          Blue   \n1        Graduate         Single  Less than $40K          Blue   \n2        Graduate        Married    $80K - $120K          Blue   \n3     High School        Unknown  Less than $40K          Blue   \n4      Uneducated        Married     $60K - $80K          Blue   \n\n   MONTHS_ON_BOOK  ...  MONTHS_INACTIVE_12_MON  CONTACTS_COUNT_12_MON  \\\n0              39  ...                       1                      3   \n1              44  ...                       1                      2   \n2              36  ...                       1                      0   \n3              34  ...                       4                      1   \n4              21  ...                       1                      0   \n\n   CREDIT_LIMIT  TOTAL_REVOLVING_BAL  AVG_OPEN_TO_BUY  TOTAL_AMT_CHNG_Q4_Q1  \\\n0       12691.0                  777          11914.0                 1.335   \n1        8256.0                  864           7392.0                 1.541   \n2        3418.0                    0           3418.0                 2.594   \n3        3313.0                 2517            796.0                 1.405   \n4        4716.0                    0           4716.0                 2.175   \n\n   TOTAL_TRANS_AMT  TOTAL_TRANS_CT  TOTAL_CT_CHNG_Q4_Q1  AVG_UTILIZATION_RATIO  \n0             1144              42                1.625                  0.061  \n1             1291              33                3.714                  0.105  \n2             1887              20                2.333                  0.000  \n3             1171              20                2.333                  0.760  \n4              816              28                2.500                  0.000  \n\n[5 rows x 21 columns]"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# This connection object is used to access your data and contains your credentials or project token.\n# You might want to remove those credentials before you share your notebook.\n\n\nimport os, ibm_db, ibm_db_dbi as dbi, pandas as pd\n\nMLops_db2_connection_dsn = 'DATABASE={};HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={uid};PWD={pwd};SECURITY=SSL'.format(\n    'bludb',\n    'b70af05b-76e4-4bca-a1f5-23dbb4c6a74e.c1ogj3sd0tgtu0lqde00.databases.appdomain.cloud',\n    32716,\n    uid='vbz23877',\n    pwd=\"\"\"N2If7ScvYPWfoA7h\"\"\"\n)\n\nMLops_db2_connection_connection = dbi.connect(MLops_db2_connection_dsn)\n   \nquery = 'SELECT * FROM \"VBZ23877\".\"BANKCHURNERS_TABLE\"'\n\ndf = pd.read_sql_query(query, con=MLops_db2_connection_connection)\ndf.head()\n\n# After use, close the database connection with the following code:\n# MLops_db2_connection_connection.close()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 3. Prepare the data"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# convert all float columns to int\ndef convert_float_to_int(df):\n    for col in df.columns:\n        if df[col].dtype == 'float64':\n            df[col] = df[col].astype('int64')\n    return df\n\ndf=convert_float_to_int(df) "
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_AGE</th>\n      <th>DEPENDENT_COUNT</th>\n      <th>MONTHS_ON_BOOK</th>\n      <th>TOTAL_RELATIONSHIP_COUNT</th>\n      <th>MONTHS_INACTIVE_12_MON</th>\n      <th>CONTACTS_COUNT_12_MON</th>\n      <th>CREDIT_LIMIT</th>\n      <th>TOTAL_REVOLVING_BAL</th>\n      <th>AVG_OPEN_TO_BUY</th>\n      <th>TOTAL_AMT_CHNG_Q4_Q1</th>\n      <th>TOTAL_TRANS_CT</th>\n      <th>TOTAL_CT_CHNG_Q4_Q1</th>\n      <th>AVG_UTILIZATION_RATIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>45</td>\n      <td>3</td>\n      <td>39</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>12691</td>\n      <td>777</td>\n      <td>11914</td>\n      <td>1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>5</td>\n      <td>44</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>8256</td>\n      <td>864</td>\n      <td>7392</td>\n      <td>1</td>\n      <td>33</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51</td>\n      <td>3</td>\n      <td>36</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3418</td>\n      <td>0</td>\n      <td>3418</td>\n      <td>2</td>\n      <td>20</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40</td>\n      <td>4</td>\n      <td>34</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>3313</td>\n      <td>2517</td>\n      <td>796</td>\n      <td>1</td>\n      <td>20</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>3</td>\n      <td>21</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4716</td>\n      <td>0</td>\n      <td>4716</td>\n      <td>2</td>\n      <td>28</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   CUSTOMER_AGE  DEPENDENT_COUNT  MONTHS_ON_BOOK  TOTAL_RELATIONSHIP_COUNT  \\\n0            45                3              39                         5   \n1            49                5              44                         6   \n2            51                3              36                         4   \n3            40                4              34                         3   \n4            40                3              21                         5   \n\n   MONTHS_INACTIVE_12_MON  CONTACTS_COUNT_12_MON  CREDIT_LIMIT  \\\n0                       1                      3         12691   \n1                       1                      2          8256   \n2                       1                      0          3418   \n3                       4                      1          3313   \n4                       1                      0          4716   \n\n   TOTAL_REVOLVING_BAL  AVG_OPEN_TO_BUY  TOTAL_AMT_CHNG_Q4_Q1  TOTAL_TRANS_CT  \\\n0                  777            11914                     1              42   \n1                  864             7392                     1              33   \n2                    0             3418                     2              20   \n3                 2517              796                     1              20   \n4                    0             4716                     2              28   \n\n   TOTAL_CT_CHNG_Q4_Q1  AVG_UTILIZATION_RATIO  \n0                    1                      0  \n1                    3                      0  \n2                    2                      0  \n3                    2                      0  \n4                    2                      0  "
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# ignoring all non numeric features for now (just to keep it simple)\nnumeric_feature_names = ['CUSTOMER_AGE', 'DEPENDENT_COUNT', 'MONTHS_ON_BOOK',  'TOTAL_RELATIONSHIP_COUNT', 'MONTHS_INACTIVE_12_MON','CONTACTS_COUNT_12_MON','CREDIT_LIMIT','TOTAL_REVOLVING_BAL','AVG_OPEN_TO_BUY','TOTAL_AMT_CHNG_Q4_Q1','TOTAL_TRANS_CT','TOTAL_CT_CHNG_Q4_Q1','AVG_UTILIZATION_RATIO']\nnumeric_features = df[numeric_feature_names]\nnumeric_features.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "train_dataset = numeric_features.sample(frac=0.8, random_state=0)\ntest_dataset = numeric_features.drop(train_dataset.index)\n\ntrain_target = train_dataset.pop('CREDIT_LIMIT')\ntest_target = test_dataset.pop('CREDIT_LIMIT')"
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": "# This generally is a good idea, but not using a normalizer for now since it causes issues when saving the model https://github.com/keras-team/keras/issues/15348\n\n# normalizer = tf.keras.layers.Normalization(axis=-1)\n# normalizer.adapt(train_dataset)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 4. Setting Up Model Monitoring"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "training_data_references = [\n                {\n                    \"id\": \"credit_limit_prediction\",\n                    \"type\": \"container\",\n                    \"connection\": {},\n                    \"location\": {\n                        \"path\": \"customer_churn_db2\"\n                    },\n                    \"schema\": {\n                        \"id\": \"training_schema\",\n                        \"fields\": [\n                            {\"name\": \"I_SHOULD_WRITE_A_SCRIPT_TO_AUTOMATE_THIS\", \"nullable\": True, \"metadata\": {}, \"type\": \"string\"}\n                        ]\n                    }\n                }\n            ]"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting ibm-aigov-facts-client\n  Downloading ibm_aigov_facts_client-1.0.28-py3-none-any.whl (106 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 106 kB 29.7 MB/s eta 0:00:01\n\u001b[?25hCollecting sqlparse>=0.3.1\n  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 3.0 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: ibm-cloud-sdk-core>=3.5.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-aigov-facts-client) (3.10.1)\nCollecting docutils<0.16,>=0.10\n  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 547 kB 96.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-aigov-facts-client) (1.3.4)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-aigov-facts-client) (1.20.3)\nRequirement already satisfied: requests<3.0,>=2.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-aigov-facts-client) (2.26.0)\nCollecting timeout-decorator\n  Downloading timeout-decorator-0.5.0.tar.gz (4.8 kB)\nCollecting mlflow-skinny==1.28.0\n  Downloading mlflow_skinny-1.28.0-py3-none-any.whl (3.5 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.5 MB 31.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: entrypoints<1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (0.3)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (3.19.1)\nRequirement already satisfied: cloudpickle<3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (1.6.0)\nRequirement already satisfied: packaging<22 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (21.3)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (8.0.4)\nRequirement already satisfied: pytz<2023 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (2021.3)\nRequirement already satisfied: importlib-metadata!=4.7.0,<5,>=3.7.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (4.8.2)\nCollecting gitpython<4,>=2.1.0\n  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 182 kB 43.1 MB/s eta 0:00:01\n\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.3.tar.gz (77 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 13.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm-aigov-facts-client) (5.4.1)\nRequirement already satisfied: pyjwt>=1.7.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (2.4.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (3.2.1)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (1.15.0)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62 kB 2.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cloud-sdk-core>=3.5.2->ibm-aigov-facts-client) (2.8.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<5,>=3.7.0->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from packaging<22->mlflow-skinny==1.28.0->ibm-aigov-facts-client) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm-aigov-facts-client) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm-aigov-facts-client) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm-aigov-facts-client) (2022.9.24)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm-aigov-facts-client) (2.0.4)\nBuilding wheels for collected packages: databricks-cli, timeout-decorator\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139103 sha256=3b7a89792908f4c0390dc9afac841e7f6d8523f0944efc856db84becd030a78c\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/7b/ef/c5/85718fa9e66dec117e42d8b4d7b8a2e40ebdec17232935615f\n  Building wheel for timeout-decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-py3-none-any.whl size=5028 sha256=2d8bb6a86e93867965380df4dc1e2594d70996b0d7a3347337b7d66eaca8ff16\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/5d/45/1d/a7d2bf8dfbdecd78983a3d422f2fe860316cfbae3f3b001ea5\nSuccessfully built databricks-cli timeout-decorator\nInstalling collected packages: smmap, gitdb, sqlparse, gitpython, databricks-cli, timeout-decorator, mlflow-skinny, docutils, ibm-aigov-facts-client\nSuccessfully installed databricks-cli-0.17.3 docutils-0.15.2 gitdb-4.0.10 gitpython-3.1.29 ibm-aigov-facts-client-1.0.28 mlflow-skinny-1.28.0 smmap-5.0.0 sqlparse-0.4.3 timeout-decorator-0.5.0\n2022/12/05 12:32:38 INFO : Experiment credit_limit_prediction does not exist, creating new experiment\n2022/12/05 12:32:38 INFO : Experiment successfully created with ID 1 and name credit_limit_prediction\n2022/12/05 12:32:39 INFO : Autolog enabled Successfully\n"
                }
            ],
            "source": "try:\n    from ibm_aigov_facts_client import AIGovFactsClient\nexcept:\n    !pip install -U ibm-aigov-facts-client\n    from ibm_aigov_facts_client import AIGovFactsClient\n    \nPROJECT_UID= os.environ['PROJECT_ID']\nCPD_URL=os.environ['RUNTIME_ENV_APSX_URL'][len('https://api.'):]\nCONTAINER_ID=PROJECT_ID\nCONTAINER_TYPE='project'\nEXPERIMENT_NAME='credit_limit_prediction'\n\nPROJECT_ACCESS_TOKEN=project.project_context.accessToken.replace('Bearer ','')\n\nfacts_client = AIGovFactsClient(api_key=API_KEY,experiment_name=EXPERIMENT_NAME,container_type=CONTAINER_TYPE,container_id=CONTAINER_ID,set_as_current_experiment=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 5. Do the actual Machine Learning"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " https://www.tensorflow.org/tutorials/keras/regression"
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": "# not using a normalizer since it causes issues when saving the model https://github.com/keras-team/keras/issues/15348\ndef get_basic_model():\n  model = tf.keras.Sequential([\n    #normalizer,\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1)\n  ])\n\n  model.compile(optimizer='adam',\n                loss=tf.keras.losses.MeanSquaredError(),\n                metrics=['accuracy'])\n  return model"
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/300\n  1/130 [..............................] - ETA: 31s - loss: 281670240.0000 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0066s). Check your callbacks.\n130/130 [==============================] - 1s 5ms/step - loss: 191990464.0000 - accuracy: 0.0000e+00 - val_loss: 170929808.0000 - val_accuracy: 0.0000e+00\nEpoch 2/300\n130/130 [==============================] - 0s 3ms/step - loss: 116546376.0000 - accuracy: 0.0000e+00 - val_loss: 43249500.0000 - val_accuracy: 0.0000e+00\nEpoch 3/300\n130/130 [==============================] - 0s 3ms/step - loss: 7366735.5000 - accuracy: 0.0000e+00 - val_loss: 613370.6875 - val_accuracy: 0.0000e+00\nEpoch 4/300\n130/130 [==============================] - 0s 3ms/step - loss: 184550.5469 - accuracy: 0.0000e+00 - val_loss: 4817.1958 - val_accuracy: 0.0000e+00\nEpoch 5/300\n130/130 [==============================] - 0s 3ms/step - loss: 2090.3406 - accuracy: 0.0000e+00 - val_loss: 1359.3800 - val_accuracy: 0.0000e+00\nEpoch 6/300\n130/130 [==============================] - 0s 3ms/step - loss: 1160.5482 - accuracy: 0.0000e+00 - val_loss: 1114.4781 - val_accuracy: 0.0000e+00\nEpoch 7/300\n130/130 [==============================] - 0s 3ms/step - loss: 971.5801 - accuracy: 0.0000e+00 - val_loss: 907.3110 - val_accuracy: 0.0000e+00\nEpoch 8/300\n130/130 [==============================] - 0s 3ms/step - loss: 820.8725 - accuracy: 0.0000e+00 - val_loss: 740.2442 - val_accuracy: 0.0000e+00\nEpoch 9/300\n130/130 [==============================] - 1s 4ms/step - loss: 674.5768 - accuracy: 0.0000e+00 - val_loss: 631.2680 - val_accuracy: 0.0000e+00\nEpoch 10/300\n130/130 [==============================] - 0s 3ms/step - loss: 561.5101 - accuracy: 0.0000e+00 - val_loss: 508.5959 - val_accuracy: 0.0000e+00\nEpoch 11/300\n130/130 [==============================] - 0s 3ms/step - loss: 469.8553 - accuracy: 0.0000e+00 - val_loss: 431.8142 - val_accuracy: 0.0000e+00\nEpoch 12/300\n130/130 [==============================] - 0s 3ms/step - loss: 390.8027 - accuracy: 0.0000e+00 - val_loss: 357.2805 - val_accuracy: 0.0000e+00\nEpoch 13/300\n130/130 [==============================] - 0s 3ms/step - loss: 335.6482 - accuracy: 0.0000e+00 - val_loss: 343.3390 - val_accuracy: 0.0000e+00\nEpoch 14/300\n130/130 [==============================] - 0s 3ms/step - loss: 285.5829 - accuracy: 0.0000e+00 - val_loss: 267.7467 - val_accuracy: 0.0000e+00\nEpoch 15/300\n130/130 [==============================] - 0s 4ms/step - loss: 245.2716 - accuracy: 0.0000e+00 - val_loss: 239.1277 - val_accuracy: 0.0000e+00\nEpoch 16/300\n130/130 [==============================] - 0s 3ms/step - loss: 221.6989 - accuracy: 0.0000e+00 - val_loss: 222.6053 - val_accuracy: 0.0000e+00\nEpoch 17/300\n130/130 [==============================] - 0s 3ms/step - loss: 193.3288 - accuracy: 0.0000e+00 - val_loss: 179.5970 - val_accuracy: 0.0000e+00\nEpoch 18/300\n130/130 [==============================] - 0s 3ms/step - loss: 175.1284 - accuracy: 0.0000e+00 - val_loss: 160.6374 - val_accuracy: 0.0000e+00\nEpoch 19/300\n130/130 [==============================] - 0s 3ms/step - loss: 155.9627 - accuracy: 0.0000e+00 - val_loss: 144.5547 - val_accuracy: 0.0000e+00\nEpoch 20/300\n130/130 [==============================] - 0s 3ms/step - loss: 142.4703 - accuracy: 0.0000e+00 - val_loss: 128.7157 - val_accuracy: 0.0000e+00\nEpoch 21/300\n130/130 [==============================] - 0s 3ms/step - loss: 123.8435 - accuracy: 0.0000e+00 - val_loss: 114.6101 - val_accuracy: 0.0000e+00\nEpoch 22/300\n130/130 [==============================] - 1s 4ms/step - loss: 118.7034 - accuracy: 0.0000e+00 - val_loss: 122.7926 - val_accuracy: 0.0000e+00\nEpoch 23/300\n130/130 [==============================] - 0s 3ms/step - loss: 99.8749 - accuracy: 0.0000e+00 - val_loss: 89.2582 - val_accuracy: 0.0000e+00\nEpoch 24/300\n130/130 [==============================] - 1s 4ms/step - loss: 89.4899 - accuracy: 0.0000e+00 - val_loss: 91.0664 - val_accuracy: 0.0000e+00\nEpoch 25/300\n130/130 [==============================] - 1s 4ms/step - loss: 77.8632 - accuracy: 0.0000e+00 - val_loss: 68.8560 - val_accuracy: 0.0000e+00\nEpoch 26/300\n130/130 [==============================] - 0s 4ms/step - loss: 66.2291 - accuracy: 0.0000e+00 - val_loss: 59.1323 - val_accuracy: 0.0000e+00\nEpoch 27/300\n130/130 [==============================] - 0s 3ms/step - loss: 57.9757 - accuracy: 0.0000e+00 - val_loss: 76.0893 - val_accuracy: 0.0000e+00\nEpoch 28/300\n130/130 [==============================] - 0s 3ms/step - loss: 50.3125 - accuracy: 0.0000e+00 - val_loss: 49.4918 - val_accuracy: 0.0000e+00\nEpoch 29/300\n130/130 [==============================] - 0s 4ms/step - loss: 44.0820 - accuracy: 0.0000e+00 - val_loss: 37.4769 - val_accuracy: 0.0000e+00\nEpoch 30/300\n130/130 [==============================] - 0s 2ms/step - loss: 35.8234 - accuracy: 0.0000e+00 - val_loss: 31.0404 - val_accuracy: 0.0000e+00\nEpoch 31/300\n130/130 [==============================] - 0s 3ms/step - loss: 31.5101 - accuracy: 0.0000e+00 - val_loss: 25.9628 - val_accuracy: 0.0000e+00\nEpoch 32/300\n130/130 [==============================] - 0s 3ms/step - loss: 26.8797 - accuracy: 0.0000e+00 - val_loss: 21.9969 - val_accuracy: 0.0000e+00\nEpoch 33/300\n130/130 [==============================] - 0s 2ms/step - loss: 23.7900 - accuracy: 0.0000e+00 - val_loss: 18.2603 - val_accuracy: 0.0000e+00\nEpoch 34/300\n130/130 [==============================] - 0s 3ms/step - loss: 18.8121 - accuracy: 0.0000e+00 - val_loss: 15.6360 - val_accuracy: 0.0000e+00\nEpoch 35/300\n130/130 [==============================] - 0s 3ms/step - loss: 17.9932 - accuracy: 0.0000e+00 - val_loss: 23.9140 - val_accuracy: 0.0000e+00\nEpoch 36/300\n130/130 [==============================] - 0s 3ms/step - loss: 15.7584 - accuracy: 0.0000e+00 - val_loss: 11.7139 - val_accuracy: 0.0000e+00\nEpoch 37/300\n130/130 [==============================] - 0s 3ms/step - loss: 12.5976 - accuracy: 0.0000e+00 - val_loss: 9.9364 - val_accuracy: 0.0000e+00\nEpoch 38/300\n130/130 [==============================] - 0s 3ms/step - loss: 10.7729 - accuracy: 0.0000e+00 - val_loss: 9.2307 - val_accuracy: 0.0000e+00\nEpoch 39/300\n130/130 [==============================] - 0s 3ms/step - loss: 10.5295 - accuracy: 0.0000e+00 - val_loss: 8.8713 - val_accuracy: 0.0000e+00\nEpoch 40/300\n130/130 [==============================] - 0s 3ms/step - loss: 8.8099 - accuracy: 0.0000e+00 - val_loss: 7.2240 - val_accuracy: 0.0000e+00\nEpoch 41/300\n130/130 [==============================] - 0s 3ms/step - loss: 7.8976 - accuracy: 0.0000e+00 - val_loss: 6.3556 - val_accuracy: 0.0000e+00\nEpoch 42/300\n130/130 [==============================] - 0s 3ms/step - loss: 7.2470 - accuracy: 0.0000e+00 - val_loss: 6.0639 - val_accuracy: 0.0000e+00\nEpoch 43/300\n130/130 [==============================] - 0s 3ms/step - loss: 7.2913 - accuracy: 0.0000e+00 - val_loss: 5.3715 - val_accuracy: 0.0000e+00\nEpoch 44/300\n130/130 [==============================] - 0s 3ms/step - loss: 7.3573 - accuracy: 0.0000e+00 - val_loss: 5.3741 - val_accuracy: 0.0000e+00\nEpoch 45/300\n130/130 [==============================] - 0s 3ms/step - loss: 6.3793 - accuracy: 0.0000e+00 - val_loss: 4.7554 - val_accuracy: 0.0000e+00\nEpoch 46/300\n130/130 [==============================] - 0s 3ms/step - loss: 5.8362 - accuracy: 0.0000e+00 - val_loss: 5.4502 - val_accuracy: 0.0000e+00\nEpoch 47/300\n130/130 [==============================] - 0s 3ms/step - loss: 9.3779 - accuracy: 0.0000e+00 - val_loss: 5.2073 - val_accuracy: 0.0000e+00\nEpoch 48/300\n130/130 [==============================] - 1s 4ms/step - loss: 6.7510 - accuracy: 0.0000e+00 - val_loss: 5.7567 - val_accuracy: 0.0000e+00\nEpoch 49/300\n130/130 [==============================] - 0s 3ms/step - loss: 5.9850 - accuracy: 0.0000e+00 - val_loss: 8.6200 - val_accuracy: 0.0000e+00\nEpoch 50/300\n130/130 [==============================] - 1s 4ms/step - loss: 5.9927 - accuracy: 0.0000e+00 - val_loss: 7.4379 - val_accuracy: 0.0000e+00\nEpoch 51/300\n130/130 [==============================] - 0s 3ms/step - loss: 11.9785 - accuracy: 0.0000e+00 - val_loss: 12.5417 - val_accuracy: 0.0000e+00\nEpoch 52/300\n130/130 [==============================] - 0s 3ms/step - loss: 6.2683 - accuracy: 0.0000e+00 - val_loss: 3.4223 - val_accuracy: 0.0000e+00\nEpoch 53/300\n130/130 [==============================] - 0s 3ms/step - loss: 4.4369 - accuracy: 0.0000e+00 - val_loss: 2.9728 - val_accuracy: 0.0000e+00\nEpoch 54/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.4227 - accuracy: 0.0000e+00 - val_loss: 5.2971 - val_accuracy: 0.0000e+00\nEpoch 55/300\n130/130 [==============================] - 0s 2ms/step - loss: 5.5978 - accuracy: 0.0000e+00 - val_loss: 4.8233 - val_accuracy: 0.0000e+00\nEpoch 56/300\n130/130 [==============================] - 0s 2ms/step - loss: 6.3060 - accuracy: 0.0000e+00 - val_loss: 2.9892 - val_accuracy: 0.0000e+00\nEpoch 57/300\n130/130 [==============================] - 0s 2ms/step - loss: 5.5288 - accuracy: 0.0000e+00 - val_loss: 2.6695 - val_accuracy: 0.0000e+00\nEpoch 58/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.7673 - accuracy: 0.0000e+00 - val_loss: 3.9040 - val_accuracy: 0.0000e+00\nEpoch 59/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.9945 - accuracy: 0.0000e+00 - val_loss: 2.4597 - val_accuracy: 0.0000e+00\nEpoch 60/300\n130/130 [==============================] - 0s 3ms/step - loss: 8.2382 - accuracy: 0.0000e+00 - val_loss: 1.7244 - val_accuracy: 0.0000e+00\nEpoch 61/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.9540 - accuracy: 0.0000e+00 - val_loss: 15.1424 - val_accuracy: 0.0000e+00\nEpoch 62/300\n130/130 [==============================] - 0s 4ms/step - loss: 45.6221 - accuracy: 0.0000e+00 - val_loss: 1.8947 - val_accuracy: 0.0000e+00\nEpoch 63/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.1643 - accuracy: 0.0000e+00 - val_loss: 2.2493 - val_accuracy: 0.0000e+00\nEpoch 64/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.7589 - accuracy: 0.0000e+00 - val_loss: 5.4040 - val_accuracy: 0.0000e+00\nEpoch 65/300\n130/130 [==============================] - 0s 2ms/step - loss: 2.7094 - accuracy: 0.0000e+00 - val_loss: 2.8775 - val_accuracy: 0.0000e+00\nEpoch 66/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.9098 - accuracy: 0.0000e+00 - val_loss: 3.0639 - val_accuracy: 0.0000e+00\nEpoch 67/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.7938 - accuracy: 0.0000e+00 - val_loss: 8.3399 - val_accuracy: 0.0000e+00\nEpoch 68/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.8721 - accuracy: 0.0000e+00 - val_loss: 0.8415 - val_accuracy: 0.0000e+00\nEpoch 69/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.9837 - accuracy: 0.0000e+00 - val_loss: 5.8577 - val_accuracy: 0.0000e+00\nEpoch 70/300\n130/130 [==============================] - 0s 3ms/step - loss: 8.9005 - accuracy: 0.0000e+00 - val_loss: 0.6209 - val_accuracy: 0.0000e+00\nEpoch 71/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.8778 - accuracy: 0.0000e+00 - val_loss: 1.9745 - val_accuracy: 0.0000e+00\nEpoch 72/300\n130/130 [==============================] - 1s 4ms/step - loss: 86.2664 - accuracy: 0.0000e+00 - val_loss: 8.0271 - val_accuracy: 0.0000e+00\nEpoch 73/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.4044 - accuracy: 0.0000e+00 - val_loss: 0.7046 - val_accuracy: 0.0000e+00\nEpoch 74/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.5603 - accuracy: 0.0000e+00 - val_loss: 0.5761 - val_accuracy: 0.0000e+00\nEpoch 75/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.0000e+00 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\nEpoch 76/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.0000e+00 - val_loss: 0.4769 - val_accuracy: 0.0000e+00\nEpoch 77/300\n130/130 [==============================] - 0s 3ms/step - loss: 4.3057 - accuracy: 0.0000e+00 - val_loss: 0.3700 - val_accuracy: 0.0000e+00\nEpoch 78/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.0000e+00 - val_loss: 0.2882 - val_accuracy: 0.0000e+00\nEpoch 79/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.0000e+00 - val_loss: 1.7944 - val_accuracy: 0.0000e+00\nEpoch 80/300\n130/130 [==============================] - 0s 3ms/step - loss: 17.6245 - accuracy: 0.0000e+00 - val_loss: 94.3720 - val_accuracy: 0.0000e+00\nEpoch 81/300\n130/130 [==============================] - 0s 3ms/step - loss: 189.4089 - accuracy: 0.0000e+00 - val_loss: 8.2662 - val_accuracy: 0.0000e+00\nEpoch 82/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.8106 - accuracy: 0.0000e+00 - val_loss: 0.1759 - val_accuracy: 0.0000e+00\nEpoch 83/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.0000e+00 - val_loss: 0.5641 - val_accuracy: 0.0000e+00\nEpoch 84/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.8003 - accuracy: 0.0000e+00 - val_loss: 0.1459 - val_accuracy: 0.0000e+00\nEpoch 85/300\n130/130 [==============================] - 0s 3ms/step - loss: 6.6195 - accuracy: 0.0000e+00 - val_loss: 2.4735 - val_accuracy: 0.0000e+00\nEpoch 86/300\n130/130 [==============================] - 0s 3ms/step - loss: 88.3279 - accuracy: 0.0000e+00 - val_loss: 305.8002 - val_accuracy: 0.0000e+00\nEpoch 87/300\n130/130 [==============================] - 0s 3ms/step - loss: 29.1880 - accuracy: 0.0000e+00 - val_loss: 8.6746 - val_accuracy: 0.0000e+00\nEpoch 88/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.1438 - accuracy: 0.0000e+00 - val_loss: 0.1399 - val_accuracy: 0.0000e+00\nEpoch 89/300\n130/130 [==============================] - 1s 4ms/step - loss: 4.0804 - accuracy: 0.0000e+00 - val_loss: 5.0627 - val_accuracy: 0.0000e+00\nEpoch 90/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.0640 - accuracy: 0.0000e+00 - val_loss: 0.7926 - val_accuracy: 0.0000e+00\nEpoch 91/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.1887 - accuracy: 0.0000e+00 - val_loss: 1.2643 - val_accuracy: 0.0000e+00\nEpoch 92/300\n130/130 [==============================] - 0s 2ms/step - loss: 6.5794 - accuracy: 0.0000e+00 - val_loss: 3.2881 - val_accuracy: 0.0000e+00\nEpoch 93/300\n130/130 [==============================] - 0s 2ms/step - loss: 7.9806 - accuracy: 0.0000e+00 - val_loss: 45.8026 - val_accuracy: 0.0000e+00\nEpoch 94/300\n130/130 [==============================] - 0s 3ms/step - loss: 458.2126 - accuracy: 0.0000e+00 - val_loss: 11.8608 - val_accuracy: 0.0000e+00\nEpoch 95/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.0572 - accuracy: 0.0000e+00 - val_loss: 0.1957 - val_accuracy: 0.0000e+00\nEpoch 96/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.0000e+00 - val_loss: 0.1330 - val_accuracy: 0.0000e+00\nEpoch 97/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.0000e+00 - val_loss: 0.0815 - val_accuracy: 0.0000e+00\nEpoch 98/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1248 - accuracy: 0.0000e+00 - val_loss: 0.0900 - val_accuracy: 0.0000e+00\nEpoch 99/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.0000e+00 - val_loss: 0.0953 - val_accuracy: 0.0000e+00\nEpoch 100/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.0000e+00 - val_loss: 0.0596 - val_accuracy: 0.0000e+00\nEpoch 101/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0519 - val_accuracy: 0.0000e+00\nEpoch 102/300\n130/130 [==============================] - 0s 4ms/step - loss: 50.6222 - accuracy: 0.0000e+00 - val_loss: 17.8346 - val_accuracy: 0.0000e+00\nEpoch 103/300\n130/130 [==============================] - 0s 3ms/step - loss: 13.8623 - accuracy: 0.0000e+00 - val_loss: 0.0917 - val_accuracy: 0.0000e+00\nEpoch 104/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.3564 - accuracy: 0.0000e+00 - val_loss: 1.5517 - val_accuracy: 0.0000e+00\nEpoch 105/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.0000e+00 - val_loss: 0.2939 - val_accuracy: 0.0000e+00\nEpoch 106/300\n130/130 [==============================] - 0s 3ms/step - loss: 235.1308 - accuracy: 0.0000e+00 - val_loss: 51.9564 - val_accuracy: 0.0000e+00\nEpoch 107/300\n130/130 [==============================] - 0s 3ms/step - loss: 28.5492 - accuracy: 0.0000e+00 - val_loss: 0.1350 - val_accuracy: 0.0000e+00\nEpoch 108/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.0000e+00 - val_loss: 0.0883 - val_accuracy: 0.0000e+00\nEpoch 109/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.0000e+00 - val_loss: 0.0581 - val_accuracy: 0.0000e+00\nEpoch 110/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 0.0000e+00 - val_loss: 0.2003 - val_accuracy: 0.0000e+00\nEpoch 111/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.5135 - accuracy: 0.0000e+00 - val_loss: 0.3895 - val_accuracy: 0.0000e+00\nEpoch 112/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 0.0000e+00 - val_loss: 0.8920 - val_accuracy: 0.0000e+00\nEpoch 113/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.2376 - accuracy: 0.0000e+00 - val_loss: 0.0367 - val_accuracy: 0.0000e+00\nEpoch 114/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.0000e+00 - val_loss: 0.0786 - val_accuracy: 0.0000e+00\nEpoch 115/300\n130/130 [==============================] - 0s 4ms/step - loss: 525.0823 - accuracy: 0.0000e+00 - val_loss: 91.4817 - val_accuracy: 0.0000e+00\nEpoch 116/300\n130/130 [==============================] - 0s 3ms/step - loss: 5.2177 - accuracy: 0.0000e+00 - val_loss: 0.0799 - val_accuracy: 0.0000e+00\nEpoch 117/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0548 - val_accuracy: 0.0000e+00\nEpoch 118/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0000e+00\nEpoch 119/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.0000e+00 - val_loss: 0.0419 - val_accuracy: 0.0000e+00\nEpoch 120/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.0000e+00 - val_loss: 0.0533 - val_accuracy: 0.0000e+00\nEpoch 121/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.0000e+00 - val_loss: 0.0288 - val_accuracy: 0.0000e+00\nEpoch 122/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.0000e+00 - val_loss: 0.0297 - val_accuracy: 0.0000e+00\nEpoch 123/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.0000e+00 - val_loss: 0.5630 - val_accuracy: 0.0000e+00\nEpoch 124/300\n130/130 [==============================] - 0s 3ms/step - loss: 115.2733 - accuracy: 0.0000e+00 - val_loss: 122.9020 - val_accuracy: 0.0000e+00\nEpoch 125/300\n130/130 [==============================] - 1s 4ms/step - loss: 26.5973 - accuracy: 0.0000e+00 - val_loss: 1.9274 - val_accuracy: 0.0000e+00\nEpoch 126/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.6219 - accuracy: 0.0000e+00 - val_loss: 0.0725 - val_accuracy: 0.0000e+00\nEpoch 127/300\n130/130 [==============================] - 0s 3ms/step - loss: 4.7281 - accuracy: 0.0000e+00 - val_loss: 4.1343 - val_accuracy: 0.0000e+00\nEpoch 128/300\n130/130 [==============================] - 0s 3ms/step - loss: 195.4140 - accuracy: 0.0000e+00 - val_loss: 168.3433 - val_accuracy: 0.0000e+00\nEpoch 129/300\n130/130 [==============================] - 0s 3ms/step - loss: 828.5298 - accuracy: 0.0000e+00 - val_loss: 396.0148 - val_accuracy: 0.0000e+00\nEpoch 130/300\n130/130 [==============================] - 0s 3ms/step - loss: 58.5400 - accuracy: 0.0000e+00 - val_loss: 0.1196 - val_accuracy: 0.0000e+00\nEpoch 131/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.0000e+00 - val_loss: 0.0624 - val_accuracy: 0.0000e+00\nEpoch 132/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0506 - val_accuracy: 0.0000e+00\nEpoch 133/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.0000e+00 - val_loss: 0.0458 - val_accuracy: 0.0000e+00\nEpoch 134/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0000e+00\nEpoch 135/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.0000e+00 - val_loss: 0.0361 - val_accuracy: 0.0000e+00\nEpoch 136/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - val_loss: 0.0611 - val_accuracy: 0.0000e+00\nEpoch 137/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.0000e+00 - val_loss: 0.0320 - val_accuracy: 0.0000e+00\nEpoch 138/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\nEpoch 139/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.0000e+00 - val_loss: 0.0280 - val_accuracy: 0.0000e+00\nEpoch 140/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.0000e+00 - val_loss: 0.0282 - val_accuracy: 0.0000e+00\nEpoch 141/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.0000e+00 - val_loss: 0.0363 - val_accuracy: 0.0000e+00\nEpoch 142/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0000e+00\nEpoch 143/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.6588 - accuracy: 0.0000e+00 - val_loss: 1.0559 - val_accuracy: 0.0000e+00\nEpoch 144/300\n130/130 [==============================] - 0s 3ms/step - loss: 55.8252 - accuracy: 0.0000e+00 - val_loss: 1.0907 - val_accuracy: 0.0000e+00\nEpoch 145/300\n130/130 [==============================] - 0s 3ms/step - loss: 102.0578 - accuracy: 0.0000e+00 - val_loss: 43.5877 - val_accuracy: 0.0000e+00\nEpoch 146/300\n130/130 [==============================] - 0s 3ms/step - loss: 54.4399 - accuracy: 0.0000e+00 - val_loss: 1115.7267 - val_accuracy: 0.0000e+00\nEpoch 147/300\n130/130 [==============================] - 0s 3ms/step - loss: 256.3547 - accuracy: 0.0000e+00 - val_loss: 7.1516 - val_accuracy: 0.0000e+00\nEpoch 148/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.0000e+00 - val_loss: 0.2479 - val_accuracy: 0.0000e+00\nEpoch 149/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.0000e+00 - val_loss: 0.0623 - val_accuracy: 0.0000e+00\nEpoch 150/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.0000e+00 - val_loss: 0.0456 - val_accuracy: 0.0000e+00\nEpoch 151/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0000e+00\nEpoch 152/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.0000e+00 - val_loss: 0.0299 - val_accuracy: 0.0000e+00\nEpoch 153/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.0000e+00 - val_loss: 0.0285 - val_accuracy: 0.0000e+00\nEpoch 154/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.0000e+00 - val_loss: 0.1269 - val_accuracy: 0.0000e+00\nEpoch 155/300\n130/130 [==============================] - 0s 3ms/step - loss: 377.0910 - accuracy: 0.0000e+00 - val_loss: 36.8601 - val_accuracy: 0.0000e+00\nEpoch 156/300\n130/130 [==============================] - 0s 3ms/step - loss: 67.7001 - accuracy: 0.0000e+00 - val_loss: 0.8174 - val_accuracy: 0.0000e+00\nEpoch 157/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.0000e+00 - val_loss: 0.1009 - val_accuracy: 0.0000e+00\nEpoch 158/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0578 - val_accuracy: 0.0000e+00\nEpoch 159/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.0000e+00 - val_loss: 0.0448 - val_accuracy: 0.0000e+00\nEpoch 160/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.0000e+00 - val_loss: 0.0351 - val_accuracy: 0.0000e+00\nEpoch 161/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.0000e+00 - val_loss: 0.0377 - val_accuracy: 0.0000e+00\nEpoch 162/300\n130/130 [==============================] - 0s 4ms/step - loss: 274.6347 - accuracy: 0.0000e+00 - val_loss: 3.9185 - val_accuracy: 0.0000e+00\nEpoch 163/300\n130/130 [==============================] - 0s 3ms/step - loss: 12.5969 - accuracy: 0.0000e+00 - val_loss: 0.1397 - val_accuracy: 0.0000e+00\nEpoch 164/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.0000e+00 - val_loss: 0.0492 - val_accuracy: 0.0000e+00\nEpoch 165/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0000e+00\nEpoch 166/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.0000e+00 - val_loss: 0.0749 - val_accuracy: 0.0000e+00\nEpoch 167/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.0000e+00 - val_loss: 0.0952 - val_accuracy: 0.0000e+00\nEpoch 168/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1094 - accuracy: 0.0000e+00 - val_loss: 0.0314 - val_accuracy: 0.0000e+00\nEpoch 169/300\n130/130 [==============================] - 0s 3ms/step - loss: 8.2806 - accuracy: 0.0000e+00 - val_loss: 4.8736 - val_accuracy: 0.0000e+00\nEpoch 170/300\n130/130 [==============================] - 0s 3ms/step - loss: 6.4013 - accuracy: 0.0000e+00 - val_loss: 11.7380 - val_accuracy: 0.0000e+00\nEpoch 171/300\n130/130 [==============================] - 0s 3ms/step - loss: 1267.2095 - accuracy: 0.0000e+00 - val_loss: 5.0325 - val_accuracy: 0.0000e+00\nEpoch 172/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.0000e+00 - val_loss: 0.1685 - val_accuracy: 0.0000e+00\nEpoch 173/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.0000e+00 - val_loss: 0.6843 - val_accuracy: 0.0000e+00\nEpoch 174/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.0000e+00 - val_loss: 0.0692 - val_accuracy: 0.0000e+00\nEpoch 175/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - val_loss: 0.0552 - val_accuracy: 0.0000e+00\nEpoch 176/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.0000e+00 - val_loss: 0.0548 - val_accuracy: 0.0000e+00\nEpoch 177/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.0000e+00 - val_loss: 0.0832 - val_accuracy: 0.0000e+00\nEpoch 178/300\n130/130 [==============================] - 1s 5ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - val_loss: 0.0326 - val_accuracy: 0.0000e+00\nEpoch 179/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.0000e+00 - val_loss: 0.0320 - val_accuracy: 0.0000e+00\nEpoch 180/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.0000e+00 - val_loss: 0.0314 - val_accuracy: 0.0000e+00\nEpoch 181/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.0000e+00 - val_loss: 0.0294 - val_accuracy: 0.0000e+00\nEpoch 182/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.0000e+00 - val_loss: 0.0336 - val_accuracy: 0.0000e+00\nEpoch 183/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.0000e+00 - val_loss: 0.0242 - val_accuracy: 0.0000e+00\nEpoch 184/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.0000e+00 - val_loss: 0.9124 - val_accuracy: 0.0000e+00\nEpoch 185/300\n130/130 [==============================] - 0s 3ms/step - loss: 22.3427 - accuracy: 0.0000e+00 - val_loss: 5369.7842 - val_accuracy: 0.0000e+00\nEpoch 186/300\n130/130 [==============================] - 0s 2ms/step - loss: 955.7534 - accuracy: 0.0000e+00 - val_loss: 0.8136 - val_accuracy: 0.0000e+00\nEpoch 187/300\n130/130 [==============================] - 1s 5ms/step - loss: 0.0867 - accuracy: 0.0000e+00 - val_loss: 0.0858 - val_accuracy: 0.0000e+00\nEpoch 188/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - val_loss: 0.0571 - val_accuracy: 0.0000e+00\nEpoch 189/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0459 - val_accuracy: 0.0000e+00\nEpoch 190/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.0000e+00 - val_loss: 0.0506 - val_accuracy: 0.0000e+00\nEpoch 191/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - val_loss: 0.0339 - val_accuracy: 0.0000e+00\nEpoch 192/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.0000e+00 - val_loss: 0.0347 - val_accuracy: 0.0000e+00\nEpoch 193/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - val_loss: 0.0298 - val_accuracy: 0.0000e+00\nEpoch 194/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0000e+00\nEpoch 195/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 0.0000e+00 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\nEpoch 196/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0289 - val_accuracy: 0.0000e+00\nEpoch 197/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.0000e+00 - val_loss: 0.0685 - val_accuracy: 0.0000e+00\nEpoch 198/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.8822 - accuracy: 0.0000e+00 - val_loss: 0.1609 - val_accuracy: 0.0000e+00\nEpoch 199/300\n130/130 [==============================] - 0s 4ms/step - loss: 32.7489 - accuracy: 0.0000e+00 - val_loss: 130.1864 - val_accuracy: 0.0000e+00\nEpoch 200/300\n130/130 [==============================] - 0s 3ms/step - loss: 137.7911 - accuracy: 0.0000e+00 - val_loss: 0.3855 - val_accuracy: 0.0000e+00\nEpoch 201/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.0000e+00 - val_loss: 0.3271 - val_accuracy: 0.0000e+00\nEpoch 202/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3391 - accuracy: 0.0000e+00 - val_loss: 0.2089 - val_accuracy: 0.0000e+00\nEpoch 203/300\n130/130 [==============================] - 0s 3ms/step - loss: 53.0615 - accuracy: 0.0000e+00 - val_loss: 9.1742 - val_accuracy: 0.0000e+00\nEpoch 204/300\n130/130 [==============================] - 0s 3ms/step - loss: 2.7077 - accuracy: 0.0000e+00 - val_loss: 14.6399 - val_accuracy: 0.0000e+00\nEpoch 205/300\n130/130 [==============================] - 0s 2ms/step - loss: 15.0309 - accuracy: 0.0000e+00 - val_loss: 1.2669 - val_accuracy: 0.0000e+00\nEpoch 206/300\n130/130 [==============================] - 0s 3ms/step - loss: 407.3702 - accuracy: 0.0000e+00 - val_loss: 285.1726 - val_accuracy: 0.0000e+00\nEpoch 207/300\n130/130 [==============================] - 0s 3ms/step - loss: 43.7135 - accuracy: 0.0000e+00 - val_loss: 0.1792 - val_accuracy: 0.0000e+00\nEpoch 208/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.0000e+00 - val_loss: 0.0589 - val_accuracy: 0.0000e+00\nEpoch 209/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.0000e+00 - val_loss: 0.0444 - val_accuracy: 0.0000e+00\nEpoch 210/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - val_loss: 0.0574 - val_accuracy: 0.0000e+00\nEpoch 211/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0359 - val_accuracy: 0.0000e+00\nEpoch 212/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - val_loss: 0.0360 - val_accuracy: 0.0000e+00\nEpoch 213/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.0000e+00 - val_loss: 0.0247 - val_accuracy: 0.0000e+00\nEpoch 214/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 0.0000e+00 - val_loss: 0.0232 - val_accuracy: 0.0000e+00\nEpoch 215/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.0000e+00 - val_loss: 0.1385 - val_accuracy: 0.0000e+00\nEpoch 216/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.2853 - accuracy: 0.0000e+00 - val_loss: 39.6987 - val_accuracy: 0.0000e+00\nEpoch 217/300\n130/130 [==============================] - 0s 3ms/step - loss: 645.2183 - accuracy: 0.0000e+00 - val_loss: 37.3986 - val_accuracy: 0.0000e+00\nEpoch 218/300\n130/130 [==============================] - 0s 2ms/step - loss: 2.8878 - accuracy: 0.0000e+00 - val_loss: 0.1939 - val_accuracy: 0.0000e+00\nEpoch 219/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.0000e+00 - val_loss: 0.0737 - val_accuracy: 0.0000e+00\nEpoch 220/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0453 - val_accuracy: 0.0000e+00\nEpoch 221/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.0000e+00 - val_loss: 0.0495 - val_accuracy: 0.0000e+00\nEpoch 222/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 0.1780 - val_accuracy: 0.0000e+00\nEpoch 223/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.0000e+00 - val_loss: 0.0908 - val_accuracy: 0.0000e+00\nEpoch 224/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - val_loss: 0.0331 - val_accuracy: 0.0000e+00\nEpoch 225/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.0000e+00 - val_loss: 0.0268 - val_accuracy: 0.0000e+00\nEpoch 226/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.0000e+00 - val_loss: 0.0466 - val_accuracy: 0.0000e+00\nEpoch 227/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.0000e+00 - val_loss: 0.0258 - val_accuracy: 0.0000e+00\nEpoch 228/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.0000e+00 - val_loss: 0.0333 - val_accuracy: 0.0000e+00\nEpoch 229/300\n130/130 [==============================] - 0s 3ms/step - loss: 30.8605 - accuracy: 0.0000e+00 - val_loss: 589.7474 - val_accuracy: 0.0000e+00\nEpoch 230/300\n130/130 [==============================] - 0s 3ms/step - loss: 314.9009 - accuracy: 0.0000e+00 - val_loss: 2.6212 - val_accuracy: 0.0000e+00\nEpoch 231/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.7220 - accuracy: 0.0000e+00 - val_loss: 0.1969 - val_accuracy: 0.0000e+00\nEpoch 232/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.0000e+00 - val_loss: 1.1934 - val_accuracy: 0.0000e+00\nEpoch 233/300\n130/130 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0000e+00\nEpoch 234/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0340 - val_accuracy: 0.0000e+00\nEpoch 235/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.0000e+00\nEpoch 236/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.0000e+00 - val_loss: 0.0310 - val_accuracy: 0.0000e+00\nEpoch 237/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.0000e+00 - val_loss: 0.0672 - val_accuracy: 0.0000e+00\nEpoch 238/300\n130/130 [==============================] - 0s 3ms/step - loss: 187.9378 - accuracy: 0.0000e+00 - val_loss: 220.0330 - val_accuracy: 0.0000e+00\nEpoch 239/300\n130/130 [==============================] - 0s 3ms/step - loss: 63.4366 - accuracy: 0.0000e+00 - val_loss: 4.3785 - val_accuracy: 0.0000e+00\nEpoch 240/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\nEpoch 241/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.0000e+00 - val_loss: 0.0614 - val_accuracy: 0.0000e+00\nEpoch 242/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\nEpoch 243/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.0000e+00 - val_loss: 0.0140 - val_accuracy: 0.0000e+00\nEpoch 244/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.0000e+00 - val_loss: 0.3472 - val_accuracy: 0.0000e+00\nEpoch 245/300\n130/130 [==============================] - 1s 5ms/step - loss: 479.2548 - accuracy: 0.0000e+00 - val_loss: 81.3647 - val_accuracy: 0.0000e+00\nEpoch 246/300\n130/130 [==============================] - 0s 4ms/step - loss: 6.2901 - accuracy: 0.0000e+00 - val_loss: 0.0704 - val_accuracy: 0.0000e+00\nEpoch 247/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0430 - val_accuracy: 0.0000e+00\nEpoch 248/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.0000e+00 - val_loss: 0.0832 - val_accuracy: 0.0000e+00\nEpoch 249/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.0000e+00 - val_loss: 0.0344 - val_accuracy: 0.0000e+00\nEpoch 250/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.0000e+00 - val_loss: 0.0411 - val_accuracy: 0.0000e+00\nEpoch 251/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0376 - val_accuracy: 0.0000e+00\nEpoch 252/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 0.0000e+00 - val_loss: 0.0250 - val_accuracy: 0.0000e+00\nEpoch 253/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0238 - val_accuracy: 0.0000e+00\nEpoch 254/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.0000e+00 - val_loss: 0.0752 - val_accuracy: 0.0000e+00\nEpoch 255/300\n130/130 [==============================] - 0s 3ms/step - loss: 6.8174 - accuracy: 0.0000e+00 - val_loss: 42.0380 - val_accuracy: 0.0000e+00\nEpoch 256/300\n130/130 [==============================] - 0s 3ms/step - loss: 349.0221 - accuracy: 0.0000e+00 - val_loss: 6.7258 - val_accuracy: 0.0000e+00\nEpoch 257/300\n130/130 [==============================] - 0s 2ms/step - loss: 2.3684 - accuracy: 0.0000e+00 - val_loss: 0.0836 - val_accuracy: 0.0000e+00\nEpoch 258/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.0000e+00 - val_loss: 1.5580 - val_accuracy: 0.0000e+00\nEpoch 259/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.0000e+00 - val_loss: 0.0353 - val_accuracy: 0.0000e+00\nEpoch 260/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.0000e+00 - val_loss: 0.0297 - val_accuracy: 0.0000e+00\nEpoch 261/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.0000e+00 - val_loss: 0.0268 - val_accuracy: 0.0000e+00\nEpoch 262/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 0.0254 - val_accuracy: 0.0000e+00\nEpoch 263/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.0000e+00 - val_loss: 0.0276 - val_accuracy: 0.0000e+00\nEpoch 264/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.4897 - accuracy: 0.0000e+00 - val_loss: 29.5904 - val_accuracy: 0.0000e+00\nEpoch 265/300\n130/130 [==============================] - 0s 3ms/step - loss: 352.5273 - accuracy: 0.0000e+00 - val_loss: 0.6256 - val_accuracy: 0.0000e+00\nEpoch 266/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.0000e+00 - val_loss: 0.0234 - val_accuracy: 0.0000e+00\nEpoch 267/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\nEpoch 268/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\nEpoch 269/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.0000e+00 - val_loss: 0.0024 - val_accuracy: 0.0000e+00\nEpoch 270/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0020 - val_accuracy: 0.0000e+00\nEpoch 271/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.0000e+00 - val_loss: 3.7239 - val_accuracy: 0.0000e+00\nEpoch 272/300\n130/130 [==============================] - 0s 4ms/step - loss: 136.1806 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\nEpoch 273/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.0000e+00 - val_loss: 0.0271 - val_accuracy: 0.0000e+00\nEpoch 274/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.0000e+00 - val_loss: 0.3307 - val_accuracy: 0.0000e+00\nEpoch 275/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.0000e+00 - val_loss: 0.5265 - val_accuracy: 0.0000e+00\nEpoch 276/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\nEpoch 277/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0000e+00 - val_loss: 0.1898 - val_accuracy: 0.0000e+00\nEpoch 278/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0343 - val_accuracy: 0.0000e+00\nEpoch 279/300\n130/130 [==============================] - 0s 3ms/step - loss: 41.7880 - accuracy: 0.0000e+00 - val_loss: 528.5292 - val_accuracy: 0.0000e+00\nEpoch 280/300\n130/130 [==============================] - 0s 2ms/step - loss: 456.1954 - accuracy: 0.0000e+00 - val_loss: 0.1947 - val_accuracy: 0.0000e+00\nEpoch 281/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.0000e+00 - val_loss: 0.0897 - val_accuracy: 0.0000e+00\nEpoch 282/300\n130/130 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0549 - val_accuracy: 0.0000e+00\nEpoch 283/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - val_loss: 0.0403 - val_accuracy: 0.0000e+00\nEpoch 284/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.0000e+00 - val_loss: 0.0807 - val_accuracy: 0.0000e+00\nEpoch 285/300\n130/130 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.0000e+00 - val_loss: 0.0315 - val_accuracy: 0.0000e+00\nEpoch 286/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.0000e+00 - val_loss: 0.0271 - val_accuracy: 0.0000e+00\nEpoch 287/300\n130/130 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.0000e+00 - val_loss: 0.0279 - val_accuracy: 0.0000e+00\nEpoch 288/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.1098 - accuracy: 0.0000e+00 - val_loss: 0.1984 - val_accuracy: 0.0000e+00\nEpoch 289/300\n130/130 [==============================] - 1s 4ms/step - loss: 44.7820 - accuracy: 0.0000e+00 - val_loss: 16.0112 - val_accuracy: 0.0000e+00\nEpoch 290/300\n130/130 [==============================] - 0s 3ms/step - loss: 282.5674 - accuracy: 0.0000e+00 - val_loss: 12.7217 - val_accuracy: 0.0000e+00\nEpoch 291/300\n130/130 [==============================] - 0s 3ms/step - loss: 1.3822 - accuracy: 0.0000e+00 - val_loss: 0.1906 - val_accuracy: 0.0000e+00\nEpoch 292/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - val_loss: 0.0415 - val_accuracy: 0.0000e+00\nEpoch 293/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0000e+00 - val_loss: 0.0354 - val_accuracy: 0.0000e+00\nEpoch 294/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 0.0000e+00 - val_loss: 0.0272 - val_accuracy: 0.0000e+00\nEpoch 295/300\n130/130 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0000e+00\nEpoch 296/300\n130/130 [==============================] - 0s 3ms/step - loss: 8.0202 - accuracy: 0.0000e+00 - val_loss: 26.7230 - val_accuracy: 0.0000e+00\nEpoch 297/300\n130/130 [==============================] - 0s 3ms/step - loss: 171.4600 - accuracy: 0.0000e+00 - val_loss: 213.0996 - val_accuracy: 0.0000e+00\nEpoch 298/300\n130/130 [==============================] - 0s 3ms/step - loss: 65.4703 - accuracy: 0.0000e+00 - val_loss: 63.3664 - val_accuracy: 0.0000e+00\nEpoch 299/300\n130/130 [==============================] - 0s 3ms/step - loss: 3.8437 - accuracy: 0.0000e+00 - val_loss: 0.3191 - val_accuracy: 0.0000e+00\nEpoch 300/300\n113/130 [=========================>....] - ETA: 0s - loss: 0.1422 - accuracy: 0.0000e+002022/12/05 12:48:14 INFO : logging results to factsheet for run_id a64adcbb008e42908f72158c0e8125ad\n2022/12/05 12:48:15 INFO : Successfully logged results to Factsheet service for run_id a64adcbb008e42908f72158c0e8125ad under asset_id: 5369e989-b952-4714-8b72-755944340817 and project_id : d08695d6-b216-4853-93c4-2daee35ddda6\n130/130 [==============================] - 2s 12ms/step - loss: 0.1277 - accuracy: 0.0000e+00 - val_loss: 0.0452 - val_accuracy: 0.0000e+00\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7feff4ce9310>"
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model = get_basic_model()\nmodel.fit(train_dataset, train_target, epochs=100, batch_size=50, validation_split = 0.2)"
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "CUSTOMER_AGE                   45\nDEPENDENT_COUNT                 3\nMONTHS_ON_BOOK                 39\nTOTAL_RELATIONSHIP_COUNT        5\nMONTHS_INACTIVE_12_MON          1\nCONTACTS_COUNT_12_MON           3\nTOTAL_REVOLVING_BAL           777\nAVG_OPEN_TO_BUY             11914\nTOTAL_AMT_CHNG_Q4_Q1            1\nTOTAL_TRANS_CT                 42\nTOTAL_CT_CHNG_Q4_Q1             1\nAVG_UTILIZATION_RATIO           0\nName: 0, dtype: int64"
                    },
                    "execution_count": 64,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "test_dataset.iloc[0]"
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[8546.831]]\n13    8547\nName: CREDIT_LIMIT, dtype: int64\n"
                }
            ],
            "source": "index=10\nprint(model.predict(test_dataset.iloc[[1]]))\nprint(test_target.iloc[[1]])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 6. Storing the model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "WTF? https://stackoverflow.com/questions/53258236/how-to-save-keras-model-in-wml-repository-in-watson-studio\n\nDo this: \nhttps://github.com/IBM/watson-machine-learning-samples/blob/master/cpd3.5/notebooks/python_sdk/deployments/keras/Use%20Keras%20to%20recognize%20hand-written%20digits.ipynb"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": "model_result_path = \"keras_model.h5\"\nmodel.save(model_result_path)"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "keras_model.h5\r\n"
                }
            ],
            "source": "!tar -zcvf keras_model.tgz keras_model.h5"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "total 72\r\ndrwxr-x--- 6 wsuser wscommon  4096 Dec  5 12:43 .\r\ndrwxrwx--- 1 wsuser wscommon  4096 Dec  5 12:32 ..\r\ndrwxr-x--- 4 wsuser wscommon  4096 Dec  5 12:43 keras_model\r\n-rw-rw---- 1 wsuser wscommon 35432 Dec  5 12:52 keras_model.h5\r\n-rw-rw---- 1 wsuser wscommon  5508 Dec  5 12:52 keras_model.tgz\r\ndrwxrwx--- 2 wsuser wscommon  4096 Dec  5 12:32 logs\r\ndrwxrwx--- 4 wsuser wscommon  4096 Dec  5 12:32 mlruns\r\ndrwxr-x--- 2 wsuser wscommon  4096 Dec  5 12:25 .virtual_documents\r\n"
                }
            ],
            "source": "! ls -la"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.1. upload the model"
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "tensorflow                    2.7.2\r\ntensorflow-addons             0.15.0\r\ntensorflow-estimator          2.7.0\r\ntensorflow-hub                0.12.0\r\ntensorflow-io-gcs-filesystem  0.23.1\r\ntensorflow-metadata           1.5.0\r\ntensorflow-probability        0.15.0\r\ntensorflow-text               2.7.3\r\n"
                }
            ],
            "source": "! pip list | grep tensorflow # get the tensorflow version for the model metadata "
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": "sofware_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\n\nmetadata = {\n            wml_client.repository.ModelMetaNames.NAME: 'credit limit model',\n            wml_client.repository.ModelMetaNames.TYPE: 'tensorflow_2.7',\n            wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\n\npublished_model = wml_client.repository.store_model(\n    model=\"keras_model.tgz\",\n    meta_props=metadata)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.2. get the model information programatically"
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "{\n  \"entity\": {\n    \"hybrid_pipeline_software_specs\": [],\n    \"software_spec\": {\n      \"id\": \"12b83a17-24d8-5082-900f-0ab31fbfd3cb\",\n      \"name\": \"runtime-22.1-py3.9\"\n    },\n    \"type\": \"tensorflow_2.7\"\n  },\n  \"metadata\": {\n    \"created_at\": \"2022-12-05T12:53:02.805Z\",\n    \"id\": \"ad59ee55-7103-47cd-99a1-ff5a3a6cf1c2\",\n    \"modified_at\": \"2022-12-05T12:53:05.270Z\",\n    \"name\": \"credit limit model\",\n    \"owner\": \"IBMid-6650024HXN\",\n    \"resource_key\": \"d916159f-af8d-41c5-8fd9-7dada48aa1c0\",\n    \"space_id\": \"d15f4a9e-e1f5-4bc2-ae89-6606bd2b62c8\"\n  },\n  \"system\": {\n    \"warnings\": []\n  }\n}\n"
                }
            ],
            "source": "import json\n\npublished_model_uid = wml_client.repository.get_model_id(published_model)\nmodel_details = wml_client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))"
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "------------------------------------  ------------------  ------------------------  --------------\nID                                    NAME                CREATED                   TYPE\nad59ee55-7103-47cd-99a1-ff5a3a6cf1c2  credit limit model  2022-12-05T12:53:02.002Z  tensorflow_2.7\n885da41b-b808-421e-9f0e-49cc0d5f6da6  credit limit model  2022-12-05T12:44:10.002Z  tensorflow_2.7\nd2569dd9-1299-4846-af65-602d78432cca  credit limit model  2022-12-05T12:37:08.002Z  tensorflow_2.7\n951eb7fb-8880-4a67-a066-63e2e26be830  credit limit model  2022-12-05T09:15:29.002Z  tensorflow_2.7\n712dca3b-ff08-4e76-b7be-5bbedcd5851b  credit limit model  2022-12-05T07:56:16.002Z  tensorflow_2.7\nea57b539-8d0c-4aed-b1e7-5402813dde05  credit limit model  2022-12-05T06:13:18.002Z  tensorflow_2.7\n------------------------------------  ------------------  ------------------------  --------------\n"
                }
            ],
            "source": "models_details = wml_client.repository.list_models()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'ad59ee55-7103-47cd-99a1-ff5a3a6cf1c2' started\n\n#######################################################################################\n\n\ninitializing\nNote: online_url is deprecated and will be removed in a future release. Use serving_urls instead.\n..\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='081870c8-c1df-479f-8d5f-4634c3defafe'\n------------------------------------------------------------------------------------------------\n\n\n"
                }
            ],
            "source": "\nmetadata = {\n    wml_client.deployments.ConfigurationMetaNames.NAME: \"Deployment of external Keras model\",\n    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ncreated_deployment = wml_client.deployments.create(published_model_uid, meta_props=metadata)"
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "metadata": {},
            "outputs": [],
            "source": "deployment_uid = wml_client.deployments.get_id(created_deployment)"
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "https://us-south.ml.cloud.ibm.com/ml/v4/deployments/081870c8-c1df-479f-8d5f-4634c3defafe/predictions\n"
                }
            ],
            "source": "scoring_endpoint = wml_client.deployments.get_scoring_href(created_deployment)\nprint(scoring_endpoint)"
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_AGE</th>\n      <th>DEPENDENT_COUNT</th>\n      <th>MONTHS_ON_BOOK</th>\n      <th>TOTAL_RELATIONSHIP_COUNT</th>\n      <th>MONTHS_INACTIVE_12_MON</th>\n      <th>CONTACTS_COUNT_12_MON</th>\n      <th>TOTAL_REVOLVING_BAL</th>\n      <th>AVG_OPEN_TO_BUY</th>\n      <th>TOTAL_AMT_CHNG_Q4_Q1</th>\n      <th>TOTAL_TRANS_CT</th>\n      <th>TOTAL_CT_CHNG_Q4_Q1</th>\n      <th>AVG_UTILIZATION_RATIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>35</td>\n      <td>3</td>\n      <td>30</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1666</td>\n      <td>6881</td>\n      <td>1</td>\n      <td>33</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "    CUSTOMER_AGE  DEPENDENT_COUNT  MONTHS_ON_BOOK  TOTAL_RELATIONSHIP_COUNT  \\\n13            35                3              30                         5   \n\n    MONTHS_INACTIVE_12_MON  CONTACTS_COUNT_12_MON  TOTAL_REVOLVING_BAL  \\\n13                       1                      3                 1666   \n\n    AVG_OPEN_TO_BUY  TOTAL_AMT_CHNG_Q4_Q1  TOTAL_TRANS_CT  \\\n13             6881                     1              33   \n\n    TOTAL_CT_CHNG_Q4_Q1  AVG_UTILIZATION_RATIO  \n13                    2                      0  "
                    },
                    "execution_count": 83,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "test_dataset.iloc[[1]]"
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1438.128662109375"
                    },
                    "execution_count": 92,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "scoring_payload = {\"input_data\": [{\"values\": test_dataset.iloc[[2]]}]}\npredictions = wml_client.deployments.score(deployment_uid, scoring_payload)\npredictions['predictions'][0]['values'][0][0][0] #pretty weird data structure :-P "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}